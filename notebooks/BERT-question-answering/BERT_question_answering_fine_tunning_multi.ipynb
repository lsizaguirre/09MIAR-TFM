{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUybaAW-LLti"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lsizaguirre/09MIAR-TFM/blob/main/notebooks/BERT-question-answering/BERT-question-answering-fine-tunning_multi.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNeQyZ6YLRO2"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://www.dropbox.com/s/crnfuz6kotkjzxa/viu_logo.png?dl=1\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "## 09MIAR - TFM\n",
        "### Question Answering\n",
        "\n",
        "*por Luis Arturo Izaguirre Viera - lsizaguirre@gmail.com*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Librerías y Entorno\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82BQfLfLLyJ6"
      },
      "source": [
        "## Instalando librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOw1i2a0ozaw",
        "outputId": "136f5271-9958-4faf-eb84-41d025623ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: tf-models-official in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.4.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.11)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.12.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.5.5.64)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.16.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.28)\n",
            "Requirement already satisfied: pyyaml<6.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.1)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.7.2)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (8.0.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.1.96)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.4)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.31.5)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.35.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2022.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.56.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.44.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.25.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.14.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (4.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-models-official) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (2019.12.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (2.4.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (0.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (21.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.7.1)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "  Attempting uninstall: tf-estimator-nightly\n",
            "    Found existing installation: tf-estimator-nightly 2.10.0.dev2022050508\n",
            "    Uninstalling tf-estimator-nightly-2.10.0.dev2022050508:\n",
            "      Successfully uninstalled tf-estimator-nightly-2.10.0.dev2022050508\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-nightly 2.10.0.dev20220427 requires tf-estimator-nightly~=2.10.0.dev, but you have tf-estimator-nightly 2.8.0.dev2021122109 which is incompatible.\u001b[0m\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (2.10.0.dev20220427)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.25.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Collecting tf-estimator-nightly~=2.10.0.dev\n",
            "  Using cached tf_estimator_nightly-2.10.0.dev2022050508-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.44.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.14.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (4.2.0)\n",
            "Requirement already satisfied: tb-nightly~=2.9.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.9.0a20220502)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (14.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (57.4.0)\n",
            "Requirement already satisfied: keras-nightly~=2.10.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.10.0.dev2022050507)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (21.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-nightly) (3.0.8)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "  Attempting uninstall: tf-estimator-nightly\n",
            "    Found existing installation: tf-estimator-nightly 2.8.0.dev2021122109\n",
            "    Uninstalling tf-estimator-nightly-2.8.0.dev2021122109:\n",
            "      Successfully uninstalled tf-estimator-nightly-2.8.0.dev2021122109\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, but you have tf-estimator-nightly 2.10.0.dev2022050508 which is incompatible.\u001b[0m\n",
            "Successfully installed tf-estimator-nightly-2.10.0.dev2022050508\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tf-models-official\n",
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2gg1pEeO6Jc"
      },
      "source": [
        "## Importando líbrerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDdYqvxPHnI8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv38eJzSH00S"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjzrCZJMJN1N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POXQerynTrtV"
      },
      "source": [
        "## Obteniendo información del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zFS9XSASHvQi",
        "outputId": "babe430b-65fb-4533-b30d-a8c9f2a45b9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.10.0-dev20220427'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJVySCYiT5C4",
        "outputId": "4744e52d-871b-44ed-a5b2-b726afd9fed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "! python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR4gT3GPTw0J",
        "outputId": "72de6ab9-d325-4095-fe38-a95231eae1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Version:  2.10.0-dev20220427\n",
            "Eager mode:  True\n",
            "Hub version:  0.12.0\n",
            "GPU is NOT AVAILABLE\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jMnqaETVyY5"
      },
      "source": [
        "## Carga de Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfdTEReCKFky",
        "outputId": "4ef3b25e-7a03-4b52-cfb5-fab47883285d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_PcOeemKcYq"
      },
      "outputs": [],
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/train-v1.1.json\",\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/vocab.txt\",\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/train-v1.1.tf_record\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XQGdnANLziM"
      },
      "outputs": [],
      "source": [
        "with tf.io.gfile.GFile(\"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "295nG2zQMYSU"
      },
      "outputs": [],
      "source": [
        "# Valor recomendado por Google dado lo complicado que podria ser procesar si la entrada es muy grande \n",
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Modelado, Fine-Tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Capa Squad\n",
        "\n",
        "Google recomienda que a la hora de crear una capa densa al final para hacer fine-tunning nos recomienda una normal truncada con una desviacion estandar pequeña."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjmLeQjiaOo5"
      },
      "outputs": [],
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] \n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgkIFb1GMT81"
      },
      "outputs": [],
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                               inputs[\"input_mask\"],\n",
        "                                               inputs[\"input_type_ids\"]])\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## Creación de la IA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEUxomxENRoJ"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 3000 # entrenando solo con 3000 lotes\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pg6EKe2daFy"
      },
      "outputs": [],
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHd5MzTdNIZq"
      },
      "outputs": [],
      "source": [
        "bert_squad = BERTSquad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0cvDjBm_KXT"
      },
      "outputs": [],
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6kG-HpzVK7v"
      },
      "outputs": [],
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True)\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN_C_WA5R_Cb",
        "outputId": "12a03579-9804-4c98-a782-5b7521ac860c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2726, 2358, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2711, ...,    0,    0,    0],\n",
              "         [ 101, 2129, 2116, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 20, 122,  29,  28], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 18, 119,  20,  28], dtype=int32)>})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "next(iter(train_dataset_light))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-iE2QFC_KRI"
      },
      "outputs": [],
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_squad.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pcFxOzwzrGZ",
        "outputId": "3f619516-097d-4695-c60a-6aab06471c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"bert_squad\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    multiple                  109482241 \n",
            "                                                                 \n",
            " bert_squad_layer (BertSquad  multiple                 1538      \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,779\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjp-_4OyTbuK"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Entrenamiento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ywelW3uaSbT",
        "outputId": "a76d9fb7-a8a7-495e-b859-2340f2bc682c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 5.7400\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 Pérdida 5.6746\n",
            "Epoch 1 Lote 100 Pérdida 5.2875\n",
            "Epoch 1 Lote 150 Pérdida 4.8045\n",
            "Epoch 1 Lote 200 Pérdida 4.4377\n",
            "Epoch 1 Lote 250 Pérdida 4.0239\n",
            "Epoch 1 Lote 300 Pérdida 3.7401\n",
            "Epoch 1 Lote 350 Pérdida 3.5493\n",
            "Epoch 1 Lote 400 Pérdida 3.3549\n",
            "Epoch 1 Lote 450 Pérdida 3.1984\n",
            "Epoch 1 Lote 500 Pérdida 3.0346\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 Pérdida 2.8972\n",
            "Epoch 1 Lote 600 Pérdida 2.7942\n",
            "Epoch 1 Lote 650 Pérdida 2.7068\n",
            "Epoch 1 Lote 700 Pérdida 2.6264\n",
            "Epoch 1 Lote 750 Pérdida 2.5467\n",
            "Epoch 1 Lote 800 Pérdida 2.4746\n",
            "Epoch 1 Lote 850 Pérdida 2.4225\n",
            "Epoch 1 Lote 900 Pérdida 2.3655\n",
            "Epoch 1 Lote 950 Pérdida 2.3103\n",
            "Epoch 1 Lote 1000 Pérdida 2.2423\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 Pérdida 2.1869\n",
            "Epoch 1 Lote 1100 Pérdida 2.1366\n",
            "Epoch 1 Lote 1150 Pérdida 2.0826\n",
            "Epoch 1 Lote 1200 Pérdida 2.0460\n",
            "Epoch 1 Lote 1250 Pérdida 2.0241\n",
            "Epoch 1 Lote 1300 Pérdida 2.0074\n",
            "Epoch 1 Lote 1350 Pérdida 1.9885\n",
            "Epoch 1 Lote 1400 Pérdida 1.9664\n",
            "Epoch 1 Lote 1450 Pérdida 1.9360\n",
            "Epoch 1 Lote 1500 Pérdida 1.9111\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 Pérdida 1.8912\n",
            "Epoch 1 Lote 1600 Pérdida 1.8656\n",
            "Epoch 1 Lote 1650 Pérdida 1.8595\n",
            "Epoch 1 Lote 1700 Pérdida 1.8528\n",
            "Epoch 1 Lote 1750 Pérdida 1.8339\n",
            "Epoch 1 Lote 1800 Pérdida 1.8199\n",
            "Epoch 1 Lote 1850 Pérdida 1.7948\n",
            "Epoch 1 Lote 1900 Pérdida 1.7661\n",
            "Epoch 1 Lote 1950 Pérdida 1.7507\n",
            "Epoch 1 Lote 2000 Pérdida 1.7415\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-5\n",
            "Epoch 1 Lote 2050 Pérdida 1.7320\n",
            "Epoch 1 Lote 2100 Pérdida 1.7204\n",
            "Epoch 1 Lote 2150 Pérdida 1.7038\n",
            "Epoch 1 Lote 2200 Pérdida 1.6988\n",
            "Epoch 1 Lote 2250 Pérdida 1.7009\n",
            "Epoch 1 Lote 2300 Pérdida 1.6944\n",
            "Epoch 1 Lote 2350 Pérdida 1.6802\n",
            "Epoch 1 Lote 2400 Pérdida 1.6676\n",
            "Epoch 1 Lote 2450 Pérdida 1.6499\n",
            "Epoch 1 Lote 2500 Pérdida 1.6323\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-6\n",
            "Epoch 1 Lote 2550 Pérdida 1.6174\n",
            "Epoch 1 Lote 2600 Pérdida 1.6133\n",
            "Epoch 1 Lote 2650 Pérdida 1.6095\n",
            "Epoch 1 Lote 2700 Pérdida 1.6028\n",
            "Epoch 1 Lote 2750 Pérdida 1.5968\n",
            "Epoch 1 Lote 2800 Pérdida 1.5911\n",
            "Epoch 1 Lote 2850 Pérdida 1.5832\n",
            "Epoch 1 Lote 2900 Pérdida 1.5667\n",
            "Epoch 1 Lote 2950 Pérdida 1.5530\n",
            "Tiempo total para entrenar 1 epoch: 8235.68862581253 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 0.5755\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 50 Pérdida 0.8598\n",
            "Epoch 2 Lote 100 Pérdida 1.0287\n",
            "Epoch 2 Lote 150 Pérdida 1.0459\n",
            "Epoch 2 Lote 200 Pérdida 1.0754\n",
            "Epoch 2 Lote 250 Pérdida 1.0320\n",
            "Epoch 2 Lote 300 Pérdida 0.9876\n",
            "Epoch 2 Lote 350 Pérdida 0.9718\n",
            "Epoch 2 Lote 400 Pérdida 0.9405\n",
            "Epoch 2 Lote 450 Pérdida 0.8916\n",
            "Epoch 2 Lote 500 Pérdida 0.8655\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 550 Pérdida 0.8320\n",
            "Epoch 2 Lote 600 Pérdida 0.8160\n",
            "Epoch 2 Lote 650 Pérdida 0.8018\n",
            "Epoch 2 Lote 700 Pérdida 0.7788\n",
            "Epoch 2 Lote 750 Pérdida 0.7476\n",
            "Epoch 2 Lote 800 Pérdida 0.7271\n",
            "Epoch 2 Lote 850 Pérdida 0.7161\n",
            "Epoch 2 Lote 900 Pérdida 0.7017\n",
            "Epoch 2 Lote 950 Pérdida 0.6887\n",
            "Epoch 2 Lote 1000 Pérdida 0.6702\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-9\n",
            "Epoch 2 Lote 1050 Pérdida 0.6545\n",
            "Epoch 2 Lote 1100 Pérdida 0.6390\n",
            "Epoch 2 Lote 1150 Pérdida 0.6245\n",
            "Epoch 2 Lote 1200 Pérdida 0.6138\n",
            "Epoch 2 Lote 1250 Pérdida 0.6088\n",
            "Epoch 2 Lote 1300 Pérdida 0.6115\n",
            "Epoch 2 Lote 1350 Pérdida 0.6160\n",
            "Epoch 2 Lote 1400 Pérdida 0.6082\n",
            "Epoch 2 Lote 1450 Pérdida 0.6044\n",
            "Epoch 2 Lote 1500 Pérdida 0.5979\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-10\n",
            "Epoch 2 Lote 1550 Pérdida 0.5936\n",
            "Epoch 2 Lote 1600 Pérdida 0.5917\n",
            "Epoch 2 Lote 1650 Pérdida 0.5983\n",
            "Epoch 2 Lote 1700 Pérdida 0.5958\n",
            "Epoch 2 Lote 1750 Pérdida 0.5927\n",
            "Epoch 2 Lote 1800 Pérdida 0.5916\n",
            "Epoch 2 Lote 1850 Pérdida 0.5831\n",
            "Epoch 2 Lote 1900 Pérdida 0.5734\n",
            "Epoch 2 Lote 1950 Pérdida 0.5692\n",
            "Epoch 2 Lote 2000 Pérdida 0.5694\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-11\n",
            "Epoch 2 Lote 2050 Pérdida 0.5718\n",
            "Epoch 2 Lote 2100 Pérdida 0.5696\n",
            "Epoch 2 Lote 2150 Pérdida 0.5719\n",
            "Epoch 2 Lote 2200 Pérdida 0.5756\n",
            "Epoch 2 Lote 2250 Pérdida 0.5842\n",
            "Epoch 2 Lote 2300 Pérdida 0.5839\n",
            "Epoch 2 Lote 2350 Pérdida 0.5808\n",
            "Epoch 2 Lote 2400 Pérdida 0.5755\n",
            "Epoch 2 Lote 2450 Pérdida 0.5690\n",
            "Epoch 2 Lote 2500 Pérdida 0.5629\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-12\n",
            "Epoch 2 Lote 2550 Pérdida 0.5597\n",
            "Epoch 2 Lote 2600 Pérdida 0.5601\n",
            "Epoch 2 Lote 2650 Pérdida 0.5636\n",
            "Epoch 2 Lote 2700 Pérdida 0.5682\n",
            "Epoch 2 Lote 2750 Pérdida 0.5718\n",
            "Epoch 2 Lote 2800 Pérdida 0.5755\n",
            "Epoch 2 Lote 2850 Pérdida 0.5768\n",
            "Epoch 2 Lote 2900 Pérdida 0.5776\n",
            "Epoch 2 Lote 2950 Pérdida 0.5772\n",
            "Tiempo total para entrenar 1 epoch: 12077.193581104279 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 0.7352\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-13\n",
            "Epoch 3 Lote 50 Pérdida 0.8803\n",
            "Epoch 3 Lote 100 Pérdida 0.9518\n",
            "Epoch 3 Lote 150 Pérdida 0.9929\n",
            "Epoch 3 Lote 200 Pérdida 1.0867\n",
            "Epoch 3 Lote 250 Pérdida 1.0437\n",
            "Epoch 3 Lote 300 Pérdida 0.9975\n",
            "Epoch 3 Lote 350 Pérdida 0.9792\n",
            "Epoch 3 Lote 400 Pérdida 0.9438\n",
            "Epoch 3 Lote 450 Pérdida 0.8929\n",
            "Epoch 3 Lote 500 Pérdida 0.8650\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-14\n",
            "Epoch 3 Lote 550 Pérdida 0.8342\n",
            "Epoch 3 Lote 600 Pérdida 0.8177\n",
            "Epoch 3 Lote 650 Pérdida 0.7986\n",
            "Epoch 3 Lote 700 Pérdida 0.7748\n",
            "Epoch 3 Lote 750 Pérdida 0.7464\n",
            "Epoch 3 Lote 800 Pérdida 0.7304\n",
            "Epoch 3 Lote 850 Pérdida 0.7171\n",
            "Epoch 3 Lote 900 Pérdida 0.7047\n",
            "Epoch 3 Lote 950 Pérdida 0.6897\n",
            "Epoch 3 Lote 1000 Pérdida 0.6722\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-15\n",
            "Epoch 3 Lote 1050 Pérdida 0.6578\n",
            "Epoch 3 Lote 1100 Pérdida 0.6403\n",
            "Epoch 3 Lote 1150 Pérdida 0.6263\n",
            "Epoch 3 Lote 1200 Pérdida 0.6135\n",
            "Epoch 3 Lote 1250 Pérdida 0.6056\n",
            "Epoch 3 Lote 1300 Pérdida 0.6102\n",
            "Epoch 3 Lote 1350 Pérdida 0.6137\n",
            "Epoch 3 Lote 1400 Pérdida 0.6107\n",
            "Epoch 3 Lote 1450 Pérdida 0.6023\n",
            "Epoch 3 Lote 1500 Pérdida 0.5973\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-16\n",
            "Epoch 3 Lote 1550 Pérdida 0.5966\n",
            "Epoch 3 Lote 1600 Pérdida 0.5950\n",
            "Epoch 3 Lote 1650 Pérdida 0.5982\n",
            "Epoch 3 Lote 1700 Pérdida 0.5977\n",
            "Epoch 3 Lote 1750 Pérdida 0.5932\n",
            "Epoch 3 Lote 1800 Pérdida 0.5911\n",
            "Epoch 3 Lote 1850 Pérdida 0.5827\n",
            "Epoch 3 Lote 1900 Pérdida 0.5742\n",
            "Epoch 3 Lote 1950 Pérdida 0.5697\n",
            "Epoch 3 Lote 2000 Pérdida 0.5717\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-17\n",
            "Epoch 3 Lote 2050 Pérdida 0.5705\n",
            "Epoch 3 Lote 2100 Pérdida 0.5688\n",
            "Epoch 3 Lote 2150 Pérdida 0.5692\n",
            "Epoch 3 Lote 2200 Pérdida 0.5737\n",
            "Epoch 3 Lote 2250 Pérdida 0.5832\n",
            "Epoch 3 Lote 2300 Pérdida 0.5836\n",
            "Epoch 3 Lote 2350 Pérdida 0.5792\n",
            "Epoch 3 Lote 2400 Pérdida 0.5752\n",
            "Epoch 3 Lote 2450 Pérdida 0.5690\n",
            "Epoch 3 Lote 2500 Pérdida 0.5636\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/ckpt_bert_squad/ckpt-18\n",
            "Epoch 3 Lote 2550 Pérdida 0.5593\n",
            "Epoch 3 Lote 2600 Pérdida 0.5602\n",
            "Epoch 3 Lote 2650 Pérdida 0.5640\n",
            "Epoch 3 Lote 2700 Pérdida 0.5685\n",
            "Epoch 3 Lote 2750 Pérdida 0.5713\n",
            "Epoch 3 Lote 2800 Pérdida 0.5766\n",
            "Epoch 3 Lote 2850 Pérdida 0.5775\n",
            "Epoch 3 Lote 2900 Pérdida 0.5768\n",
            "Epoch 3 Lote 2950 Pérdida 0.5754\n",
            "Tiempo total para entrenar 1 epoch: 11739.000829935074 segs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## Preparación de la evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "outputs": [],
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZTw1rMcPbeY",
        "outputId": "afdd6104-0b40-4eff-9418-b55e659c732b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qas_id: 56be4db0acb8001400a502ec, question_text: Which NFL team represented the AFC at Super Bowl 50?, doc_tokens: [Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "eval_examples[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "outputs": [],
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "outputs": [],
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "outputs": [],
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "outputs": [],
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "outputs": [],
      "source": [
        "eval_writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SHf_IKSmz43p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Llevar a cabo las prediccioness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX"
      },
      "source": [
        "Definir un cierto tipo de colección (como un diccionario)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyWOUKaDP0H0"
      },
      "outputs": [],
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4"
      },
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BScaA0SZQgQW"
      },
      "outputs": [],
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C"
      },
      "source": [
        "Hacemos nuestras predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqD78Xdjrvpn",
        "outputId": "4fd94a19-9929-4af4-cf71-cf912681bc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ]
        }
      ],
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs  \n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr"
      },
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionará con el script de evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esLdRf7uM3Lz"
      },
      "outputs": [],
      "source": [
        "output_prediction_file = \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/predictions.json\"\n",
        "output_nbest_file = \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/gdrive/MyDrive/VIU-MIAR/09MIAR-TFM/BERT-question-answering/dataset/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## Predicción casera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### Creación del diccionario de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo"
      },
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenización, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando qué token se corresponde con qué palabra (ver la función `tokenize_context()` para más detalle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u"
      },
      "source": [
        "#### Útiles varios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "outputs": [],
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_fCe_hLC12"
      },
      "outputs": [],
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      },
      "outputs": [],
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      },
      "outputs": [],
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. También almacena, para cada nuevo token, la palabra original\n",
        "    del parámetro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8qreqEURUOP"
      },
      "outputs": [],
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2sPGXxsYUsY"
      },
      "outputs": [],
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Toma una pregunta y un contexto como cadenas y devuelva un diccionario con \n",
        "    los 3 elementos necesarios para el modelo. También devuelva context_words, \n",
        "    la correspondencia context_tok a context_word ids y la longitud de \n",
        "    question_tok que necesitaremos más adelante.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # En este caso el modelo ha sido entrenado para entradas de una longitud máxima de 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT"
      },
      "source": [
        "#### Creación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQlM-M8rMklA"
      },
      "outputs": [],
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeB29SCNNQ1M"
      },
      "outputs": [],
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v5nLMNjZufe"
      },
      "outputs": [],
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X"
      },
      "source": [
        "### Predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcJgDa9gVShl"
      },
      "outputs": [],
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V40U1NzwgJpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc0f4bf-4ddf-4392-bb36-075c99619b0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 384), dtype=float32, numpy=\n",
              "array([[-4.8681197 , -6.8374324 , -6.7977743 , -6.9242225 , -6.3793073 ,\n",
              "        -6.845343  , -6.996314  , -6.3063245 , -3.831511  , -4.928098  ,\n",
              "        -6.1549954 , -5.2746267 , -6.220543  , -6.460893  , -6.995262  ,\n",
              "        -6.4555097 , -6.1558867 , -5.703503  , -6.9461884 , -4.621713  ,\n",
              "        -6.6177297 , -6.563241  , -6.3777204 , -5.088703  , -6.949404  ,\n",
              "        -5.118546  , -6.6665134 , -6.6342807 , -2.7972443 , -6.308145  ,\n",
              "        -3.6453466 , -6.528621  , -4.6473436 , -6.065271  , -5.611608  ,\n",
              "        -5.5659857 , -5.7484603 , -6.7741914 , -6.8635736 , -6.6369524 ,\n",
              "        -6.830852  , -5.8236003 , -7.039069  , -5.995853  , -7.1023784 ,\n",
              "        -6.5995407 , -3.497672  , -4.5487876 , -6.6904054 , -3.2950149 ,\n",
              "        -5.9853954 , -5.188611  , -6.0134597 , -6.390878  , -6.200931  ,\n",
              "        -3.4054515 , -6.5749865 , -5.034351  , -6.2924137 , -5.98198   ,\n",
              "        -6.160398  , -5.524634  , -4.8826494 , -6.5172696 , -6.886624  ,\n",
              "        -6.8748717 , -3.6873968 , -3.9431689 , -4.877883  , -4.5075955 ,\n",
              "         2.523409  , -3.6225343 ,  0.37557906, -4.574609  , -2.884056  ,\n",
              "        -2.2662406 , -3.481987  ,  0.6463127 , -4.0943427 , -5.9927993 ,\n",
              "        -6.3108144 , -6.244156  , -4.7907043 , -3.8138607 , -3.9207098 ,\n",
              "        -5.313874  , -6.140432  , -4.684112  , -6.6593256 , -6.2103653 ,\n",
              "        -6.538344  , -6.6350136 , -5.5299006 , -5.3788233 , -6.711516  ,\n",
              "        -6.2105365 , -3.922631  , -6.4640775 , -6.1378303 , -4.8209863 ,\n",
              "        -6.057174  , -4.5554833 , -6.579675  , -6.5367217 , -5.300696  ,\n",
              "        -5.864159  , -6.3083415 , -7.068044  , -7.0866036 , -7.0868564 ,\n",
              "        -7.0700274 , -7.0724707 , -7.0678744 , -7.0608444 , -7.0627384 ,\n",
              "        -7.049294  , -7.044214  , -7.034302  , -7.0239034 , -7.027228  ,\n",
              "        -7.0339575 , -7.0380397 , -7.0515738 , -7.0662932 , -7.0661283 ,\n",
              "        -7.080913  , -7.0789433 , -7.0896335 , -7.0983834 , -7.092108  ,\n",
              "        -7.1139565 , -7.086064  , -7.079994  , -7.0729923 , -7.070398  ,\n",
              "        -7.065434  , -7.0694156 , -7.0736537 , -7.055027  , -7.0631585 ,\n",
              "        -7.067959  , -7.062153  , -7.0730567 , -7.0727077 , -7.0633216 ,\n",
              "        -7.075938  , -7.078703  , -7.063954  , -7.068708  , -7.0600758 ,\n",
              "        -7.055104  , -7.0530562 , -7.0546026 , -7.049768  , -7.0571346 ,\n",
              "        -7.074745  , -7.081233  , -7.076801  , -7.1061416 , -7.1224236 ,\n",
              "        -7.1199646 , -7.126896  , -7.117582  , -7.1250453 , -7.1293454 ,\n",
              "        -7.121111  , -7.1493855 , -7.1369944 , -7.144937  , -7.132232  ,\n",
              "        -7.140063  , -7.120989  , -7.1099763 , -7.0965114 , -7.101873  ,\n",
              "        -7.086098  , -7.079518  , -7.0695076 , -7.0559483 , -7.0538855 ,\n",
              "        -7.055777  , -7.0443215 , -7.039759  , -7.037703  , -7.0389113 ,\n",
              "        -7.0473104 , -7.065741  , -7.066513  , -7.076466  , -7.084429  ,\n",
              "        -7.09947   , -7.104009  , -7.0994606 , -7.1063604 , -7.1115403 ,\n",
              "        -7.1209598 , -7.117042  , -7.1030498 , -7.1218734 , -7.095782  ,\n",
              "        -7.0916433 , -7.0797305 , -7.0824094 , -7.093867  , -7.095445  ,\n",
              "        -7.090883  , -7.0974855 , -7.086517  , -7.083659  , -7.0703197 ,\n",
              "        -7.0697083 , -7.055201  , -7.0659685 , -7.0548873 , -7.051811  ,\n",
              "        -7.0405593 , -7.0407    , -7.0505204 , -7.0559793 , -7.0490685 ,\n",
              "        -7.054437  , -7.067601  , -7.076661  , -7.0790405 , -7.077162  ,\n",
              "        -7.086098  , -7.0830865 , -7.0862    , -7.0805697 , -7.0730433 ,\n",
              "        -7.0842834 , -7.0900726 , -7.0933957 , -7.099769  , -7.1188393 ,\n",
              "        -7.116223  , -7.1290574 , -7.139522  , -7.1432257 , -7.1344533 ,\n",
              "        -7.125799  , -7.1368003 , -7.1225867 , -7.11548   , -7.1201644 ,\n",
              "        -7.1153398 , -7.1057096 , -7.1169043 , -7.0987096 , -7.0867887 ,\n",
              "        -7.096547  , -7.0902886 , -7.0801153 , -7.0873165 , -7.0803514 ,\n",
              "        -7.0747476 , -7.065656  , -7.06791   , -7.0688376 , -7.057363  ,\n",
              "        -7.069628  , -7.060446  , -7.05157   , -7.0635023 , -7.0873475 ,\n",
              "        -7.080108  , -7.090219  , -7.1040606 , -7.1007214 , -7.1240945 ,\n",
              "        -7.118457  , -7.1415567 , -7.136913  , -7.12556   , -7.1491013 ,\n",
              "        -7.134257  , -7.144234  , -7.130098  , -7.1190667 , -7.1556616 ,\n",
              "        -7.1481004 , -7.1214705 , -7.1265593 , -7.1346292 , -7.1349587 ,\n",
              "        -7.119611  , -7.108729  , -7.1044273 , -7.0990505 , -7.113437  ,\n",
              "        -7.121263  , -7.1002126 , -7.1222644 , -7.104734  , -7.072333  ,\n",
              "        -7.087881  , -7.08398   , -7.0840297 , -7.10177   , -7.0585523 ,\n",
              "        -7.073401  , -7.090802  , -7.130711  , -7.1484017 , -7.1309743 ,\n",
              "        -7.128119  , -7.1356273 , -7.128231  , -7.1571302 , -7.133182  ,\n",
              "        -7.149279  , -7.1507354 , -7.107661  , -7.158733  , -7.1298294 ,\n",
              "        -7.1198273 , -7.1369653 , -7.1354713 , -7.0739284 , -7.0754094 ,\n",
              "        -7.0738406 , -7.0692015 , -7.071337  , -7.084026  , -7.0792866 ,\n",
              "        -7.0712743 , -7.0956225 , -7.0975966 , -7.1320057 , -7.1379795 ,\n",
              "        -7.0914707 , -7.125588  , -7.1102877 , -7.153113  , -7.152027  ,\n",
              "        -7.098812  , -7.0971427 , -7.151263  , -7.1681523 , -7.098714  ,\n",
              "        -7.094559  , -7.108558  , -7.0932236 , -7.1293163 , -7.1075397 ,\n",
              "        -7.0779023 , -7.0757666 , -7.0669165 , -7.0620036 , -7.05898   ,\n",
              "        -7.073201  , -7.0529714 , -7.0511017 , -7.047956  , -7.0612206 ,\n",
              "        -7.0833    , -7.1094127 , -7.091449  , -7.13109   , -7.1522236 ,\n",
              "        -7.140265  , -7.1538587 , -7.1524334 , -7.1612897 , -7.1609473 ,\n",
              "        -7.1420703 , -7.130612  , -7.1426973 , -7.133598  , -7.1169167 ,\n",
              "        -7.1281667 , -7.092964  , -7.0947247 , -7.082251  , -7.074659  ,\n",
              "        -7.092536  , -7.068335  , -7.0596957 , -7.080092  , -7.073239  ,\n",
              "        -7.0652723 , -7.0705075 , -7.0770226 , -7.0668163 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "start_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      },
      "source": [
        "### Interpretación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      },
      "source": [
        "Eliminamos los id correspondientes a la pregunta y el token [\"SEP\"]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfwBJfsSTwRn"
      },
      "outputs": [],
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5sTrtSmgVYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577f8b13-476f-41c5-b960-9e66d3d95056"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.831511  , -4.928098  , -6.1549954 , -5.2746267 , -6.220543  ,\n",
              "       -6.460893  , -6.995262  , -6.4555097 , -6.1558867 , -5.703503  ,\n",
              "       -6.9461884 , -4.621713  , -6.6177297 , -6.563241  , -6.3777204 ,\n",
              "       -5.088703  , -6.949404  , -5.118546  , -6.6665134 , -6.6342807 ,\n",
              "       -2.7972443 , -6.308145  , -3.6453466 , -6.528621  , -4.6473436 ,\n",
              "       -6.065271  , -5.611608  , -5.5659857 , -5.7484603 , -6.7741914 ,\n",
              "       -6.8635736 , -6.6369524 , -6.830852  , -5.8236003 , -7.039069  ,\n",
              "       -5.995853  , -7.1023784 , -6.5995407 , -3.497672  , -4.5487876 ,\n",
              "       -6.6904054 , -3.2950149 , -5.9853954 , -5.188611  , -6.0134597 ,\n",
              "       -6.390878  , -6.200931  , -3.4054515 , -6.5749865 , -5.034351  ,\n",
              "       -6.2924137 , -5.98198   , -6.160398  , -5.524634  , -4.8826494 ,\n",
              "       -6.5172696 , -6.886624  , -6.8748717 , -3.6873968 , -3.9431689 ,\n",
              "       -4.877883  , -4.5075955 ,  2.523409  , -3.6225343 ,  0.37557906,\n",
              "       -4.574609  , -2.884056  , -2.2662406 , -3.481987  ,  0.6463127 ,\n",
              "       -4.0943427 , -5.9927993 , -6.3108144 , -6.244156  , -4.7907043 ,\n",
              "       -3.8138607 , -3.9207098 , -5.313874  , -6.140432  , -4.684112  ,\n",
              "       -6.6593256 , -6.2103653 , -6.538344  , -6.6350136 , -5.5299006 ,\n",
              "       -5.3788233 , -6.711516  , -6.2105365 , -3.922631  , -6.4640775 ,\n",
              "       -6.1378303 , -4.8209863 , -6.057174  , -4.5554833 , -6.579675  ,\n",
              "       -6.5367217 , -5.300696  , -5.864159  , -6.3083415 , -7.068044  ,\n",
              "       -7.0866036 , -7.0868564 , -7.0700274 , -7.0724707 , -7.0678744 ,\n",
              "       -7.0608444 , -7.0627384 , -7.049294  , -7.044214  , -7.034302  ,\n",
              "       -7.0239034 , -7.027228  , -7.0339575 , -7.0380397 , -7.0515738 ,\n",
              "       -7.0662932 , -7.0661283 , -7.080913  , -7.0789433 , -7.0896335 ,\n",
              "       -7.0983834 , -7.092108  , -7.1139565 , -7.086064  , -7.079994  ,\n",
              "       -7.0729923 , -7.070398  , -7.065434  , -7.0694156 , -7.0736537 ,\n",
              "       -7.055027  , -7.0631585 , -7.067959  , -7.062153  , -7.0730567 ,\n",
              "       -7.0727077 , -7.0633216 , -7.075938  , -7.078703  , -7.063954  ,\n",
              "       -7.068708  , -7.0600758 , -7.055104  , -7.0530562 , -7.0546026 ,\n",
              "       -7.049768  , -7.0571346 , -7.074745  , -7.081233  , -7.076801  ,\n",
              "       -7.1061416 , -7.1224236 , -7.1199646 , -7.126896  , -7.117582  ,\n",
              "       -7.1250453 , -7.1293454 , -7.121111  , -7.1493855 , -7.1369944 ,\n",
              "       -7.144937  , -7.132232  , -7.140063  , -7.120989  , -7.1099763 ,\n",
              "       -7.0965114 , -7.101873  , -7.086098  , -7.079518  , -7.0695076 ,\n",
              "       -7.0559483 , -7.0538855 , -7.055777  , -7.0443215 , -7.039759  ,\n",
              "       -7.037703  , -7.0389113 , -7.0473104 , -7.065741  , -7.066513  ,\n",
              "       -7.076466  , -7.084429  , -7.09947   , -7.104009  , -7.0994606 ,\n",
              "       -7.1063604 , -7.1115403 , -7.1209598 , -7.117042  , -7.1030498 ,\n",
              "       -7.1218734 , -7.095782  , -7.0916433 , -7.0797305 , -7.0824094 ,\n",
              "       -7.093867  , -7.095445  , -7.090883  , -7.0974855 , -7.086517  ,\n",
              "       -7.083659  , -7.0703197 , -7.0697083 , -7.055201  , -7.0659685 ,\n",
              "       -7.0548873 , -7.051811  , -7.0405593 , -7.0407    , -7.0505204 ,\n",
              "       -7.0559793 , -7.0490685 , -7.054437  , -7.067601  , -7.076661  ,\n",
              "       -7.0790405 , -7.077162  , -7.086098  , -7.0830865 , -7.0862    ,\n",
              "       -7.0805697 , -7.0730433 , -7.0842834 , -7.0900726 , -7.0933957 ,\n",
              "       -7.099769  , -7.1188393 , -7.116223  , -7.1290574 , -7.139522  ,\n",
              "       -7.1432257 , -7.1344533 , -7.125799  , -7.1368003 , -7.1225867 ,\n",
              "       -7.11548   , -7.1201644 , -7.1153398 , -7.1057096 , -7.1169043 ,\n",
              "       -7.0987096 , -7.0867887 , -7.096547  , -7.0902886 , -7.0801153 ,\n",
              "       -7.0873165 , -7.0803514 , -7.0747476 , -7.065656  , -7.06791   ,\n",
              "       -7.0688376 , -7.057363  , -7.069628  , -7.060446  , -7.05157   ,\n",
              "       -7.0635023 , -7.0873475 , -7.080108  , -7.090219  , -7.1040606 ,\n",
              "       -7.1007214 , -7.1240945 , -7.118457  , -7.1415567 , -7.136913  ,\n",
              "       -7.12556   , -7.1491013 , -7.134257  , -7.144234  , -7.130098  ,\n",
              "       -7.1190667 , -7.1556616 , -7.1481004 , -7.1214705 , -7.1265593 ,\n",
              "       -7.1346292 , -7.1349587 , -7.119611  , -7.108729  , -7.1044273 ,\n",
              "       -7.0990505 , -7.113437  , -7.121263  , -7.1002126 , -7.1222644 ,\n",
              "       -7.104734  , -7.072333  , -7.087881  , -7.08398   , -7.0840297 ,\n",
              "       -7.10177   , -7.0585523 , -7.073401  , -7.090802  , -7.130711  ,\n",
              "       -7.1484017 , -7.1309743 , -7.128119  , -7.1356273 , -7.128231  ,\n",
              "       -7.1571302 , -7.133182  , -7.149279  , -7.1507354 , -7.107661  ,\n",
              "       -7.158733  , -7.1298294 , -7.1198273 , -7.1369653 , -7.1354713 ,\n",
              "       -7.0739284 , -7.0754094 , -7.0738406 , -7.0692015 , -7.071337  ,\n",
              "       -7.084026  , -7.0792866 , -7.0712743 , -7.0956225 , -7.0975966 ,\n",
              "       -7.1320057 , -7.1379795 , -7.0914707 , -7.125588  , -7.1102877 ,\n",
              "       -7.153113  , -7.152027  , -7.098812  , -7.0971427 , -7.151263  ,\n",
              "       -7.1681523 , -7.098714  , -7.094559  , -7.108558  , -7.0932236 ,\n",
              "       -7.1293163 , -7.1075397 , -7.0779023 , -7.0757666 , -7.0669165 ,\n",
              "       -7.0620036 , -7.05898   , -7.073201  , -7.0529714 , -7.0511017 ,\n",
              "       -7.047956  , -7.0612206 , -7.0833    , -7.1094127 , -7.091449  ,\n",
              "       -7.13109   , -7.1522236 , -7.140265  , -7.1538587 , -7.1524334 ,\n",
              "       -7.1612897 , -7.1609473 , -7.1420703 , -7.130612  , -7.1426973 ,\n",
              "       -7.133598  , -7.1169167 , -7.1281667 , -7.092964  , -7.0947247 ,\n",
              "       -7.082251  , -7.074659  , -7.092536  , -7.068335  , -7.0596957 ,\n",
              "       -7.080092  , -7.073239  , -7.0652723 , -7.0705075 , -7.0770226 ,\n",
              "       -7.0668163 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "start_logits_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf"
      },
      "source": [
        "Primero la interpretación sencilla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      },
      "outputs": [],
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsjHyosnggSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac476c9d-91fe-4405-e509-6f67832eb312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "end_word_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse"
      },
      "source": [
        "\"Avanzada\" - nos aseguramos que el principio de la respuesta este antes del final:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      },
      "outputs": [],
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9KEiFHPXXeM"
      },
      "outputs": [],
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijLjfDICgqA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f5d156-f32d-4cbf-a2ea-cc5670b83c3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "start_word_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP"
      },
      "source": [
        "Final de la respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f32a180-809a-44cb-f123-eec4749b3bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ]
        }
      ],
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e8519821-1914-4b35-b97f-782b008f9962"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXMJwopGRzM"
      },
      "source": [
        "#### Bueno pero no tanto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "d0d2acb3-0e2e-4e1e-95f7-e2665860c100"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>WHAT YEAR WAS REAL MADRID FOUNDED?</h2>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<blockquote> \n",
              "Founded on 6 March 1902 as Madrid Football Club, the club has traditionally worn a white home kit since inception. The honorific title real is Spanish for \"royal\" and was bestowed to the club by King Alfonso XIII in 1920 together with the royal crown in the emblem. The team has Santiago Bernabéu Stadium <mark>in downtown Madrid since 1947. Unlike most</mark> European sporting entities, Real Madrid's members (socios) have owned and operated the club throughout its history.\n",
              "\n",
              "The club was estimated to be worth €3.8 billion ($4.2 billion) in 2019, and it was the second highest-earning football club in the world, with a big annual revenue. The club is one of the most widely supported teams in the world.[8] Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably El Clásico with Barcelona and El Derbi Madrileño with Atlético Madrid.\n",
              " </blockquote>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "my_context = '''\n",
        "Founded on 6 March 1902 as Madrid Football Club, the club has traditionally worn a white home kit since inception. The honorific title real is Spanish for \"royal\" and was bestowed to the club by King Alfonso XIII in 1920 together with the royal crown in the emblem. The team has Santiago Bernabéu Stadium in downtown Madrid since 1947. Unlike most European sporting entities, Real Madrid's members (socios) have owned and operated the club throughout its history.\n",
        "\n",
        "The club was estimated to be worth €3.8 billion ($4.2 billion) in 2019, and it was the second highest-earning football club in the world, with a big annual revenue. The club is one of the most widely supported teams in the world.[8] Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably El Clásico with Barcelona and El Derbi Madrileño with Atlético Madrid.\n",
        "'''\n",
        "\n",
        "my_question = '''what year was real madrid founded?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BERT-question-answering-fine-tunning_multi.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}