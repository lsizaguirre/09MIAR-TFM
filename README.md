# 09MIAR-TFM

MÁSTER UNIVERSITARIO EN INTELIGENCIA ARTIFICIAL

*Implementación y evaluación de aplicaciones usando BERT, para los problemas de análisis de sentimiento y respuesta a preguntas.*

<!-- Resumen -->
## Resumen

BERT, siglas para Bidirectional Encoder Representations from Transformers es un mo-
delo de lenguaje preentrenado basado en la arquitectura de transformers que desde su
aparición en octubre del 2018 ha avanzado en de forma acelerada en el estado del arte de
diversas tareas comprendidas en el campo del procesamiento del lenguaje natural, como
el resumen de textos, clasificación, similaridad semántica, entre otras.

El objetivo de este trabajo de investigación es evaluar el uso, la facilidad de implemen-
tación y el rendimiento que puede obtener este modelo en la solución de tareas para las
que en un principio no fue entrenado.

Se eligió trabajar con dos tareas, la primera de ellas fue un problema de clasificación
a través del análisis de sentimiento sobre un conjunto de datos que contiene reseñas de
películas y la segunda un problema de comprensión lectora donde se intenta identificar una
respuesta dada una pregunta y un contexto. Para resolver ambos problemas se realizó un
proceso de ajuste o fine-tunning sobre el modelo base de BERT.

Los resultados mostraron además de la facilidad de implementación, un excelente ren-
dimiento al compararlo con los modelos que definen el estado del arte para cada una
de estas tareas. Con un ligero proceso de ajuste se logró tener para el primer problema
una precisión cercana al 92 % y para el problema de preguntas y respuestas un score F1
cercano al 77 %.

Los resultados sugieren que BERT es un excelente punto de partida para la cons-
trucción de soluciones a distintos problemas en el campo del procesamiento de lenguaje
natural.


<!-- CONTACT -->
## Contacto

Luis Izaguirre - [me@LinkedIn](https://www.linkedin.com/in/luisarturoizaguirre/) - lsizaguirre@gmail.com
