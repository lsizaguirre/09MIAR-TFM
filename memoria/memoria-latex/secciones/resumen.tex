\cleardoublepage

\chapter*{Resumen}
\label{chpater-resumen}
\addcontentsline{toc}{chapter}{Resumen}

\textbf{BERT}, siglas para \textit{\textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations from \textbf{T}ransformers} es un modelo de lenguaje preentrenado basado en la arquitectura de transformers que desde su aparición en octubre del 2018 ha avanzado en de forma acelerada en el estado del arte de diversas tareas comprendidas en el campo del procesamiento del lenguaje natural, como el resumen de textos, clasificación, similaridad semántica, entre otras.

El objetivo de este trabajo de investigación es evaluar el uso, la facilidad de implementación y el rendimiento que puede obtener este modelo en la solución de tareas para las que en un principio no fue entrenado.

Se eligió trabajar con dos tareas, la primera de ellas fue un problema de clasificación a través del análisis de sentimiento sobre un conjunto de datos que contiene reseñas de películas y la segunda un problema de comprensión lectora donde se intenta identificar una respuesta dada una pregunta y un contexto. Para resolver ambos problemas se realizó un proceso de ajuste o fine-tunning sobre el modelo base de BERT. 

Los resultados mostraron además de la facilidad de implementación, un excelente rendimiento al compararlo con los modelos que definen el estado del arte para cada una de estas tareas. Con un ligero proceso de ajuste se logró tener para el primer problema una precisión cercana al 92\% y para el problema de preguntas y respuestas un score F1 cercano al 77\%.

Los resultados sugieren que BERT es un excelente punto de partida para la construcción de soluciones a distintos problemas en el campo del procesamiento de lenguaje natural. 