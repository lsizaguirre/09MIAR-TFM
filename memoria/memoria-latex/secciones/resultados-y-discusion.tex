% RESULTADOS Y DISCUSIÓN 

\cleardoublepage

\chapter{Resultados y Discusión}
\label{chapter-resultados-y-discusion}

Los resultados obtenidos en ambos experimentos realizados en este trabajo y descritos en la sección \ref{chapter-metodologia-desarrollo}, sugieren que BERT es una línea de base sólida para el desarrollo de soluciones a distintas tareas de procesamiento del lenguaje natural. Las soluciones que fueron propuestas, tanto al problema de clasificación como en el problema de respuesta a preguntas, se basaron en ligeros ajustes al modelo preentrenado obteniendo en ambos casos resultados muy cercanos a los que representan el estado del arte de estos problemas.

Esta capacidad de adaptación de BERT y la obtención de buenos resultados en distintos problemas es también confirmada por otros autores en distintos estudios. Como en el trabajo realizado por \cite{csedu20}, quienes compararon el desempeño de BERT con otro modelo similar (XLNet) en un problema de preguntas y respuestas. 

\cite{Topal_2021_exploring_https://doi.org/10.48550/arxiv.2102.08036}, compararon 3 modelos basados en transformers, BERT, XLNet y GPT-3. En el trabajo mencionan las implicaciones significativas que han tenido todos estos modelos en el campo del procesamiento natural del lenguaje y de los buenos resultados que se obtienen a partir de ellos incluso cuando no son ajustados para propósitos específicos. 

Es importante por último resaltar que los ajustes realizados en cada uno de los experimentos fueron mínimos y que para el problema de respuesta a preguntas no se usó la totalidad del conjunto de entrenamiento por el costo de procesamiento. A pesar de haber obtenido buenos resultados, la oportunidad de mejora es amplia.