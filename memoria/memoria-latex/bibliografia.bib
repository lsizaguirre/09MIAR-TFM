@inproceedings{goodfellow2014generative,
    title       ={Generative adversarial nets},
    author      ={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle   ={Advances in neural information processing systems},
    pages       ={2672--2680},
    year        ={2014}
}

@Inbook{Sabharwal2021,
    author="Sabharwal, Navin
    and Agrawal, Amit",
    title="BERT Model Applications: Question Answering System",
    bookTitle="Hands-on Question Answering Systems with BERT: Applications in Neural Networks and Natural Language Processing ",
    year="2021",
    publisher="Apress",
    address="Berkeley, CA",
    pages="97--137",
    abstract="We are surrounded by massive amounts of information present in the form of documents, images, blogs, websites, and more. In most cases, we always look for a direct answer instead of reading the entirety of lengthy documents. Question answering systems are generally being used for this purpose. These systems scan thorough a corpus of documents and provide you with the relevant answer or paragraph. It is part of the computer science discipline in the field of information retrieval and NLP, which focuses on building systems that automatically extract an answer to questions posed by humans or machines in a natural language.",
    isbn="978-1-4842-6664-9",
    doi="10.1007/978-1-4842-6664-9_5",
    url="https://doi.org/10.1007/978-1-4842-6664-9_5"
}

@article{10.1145/3185045,
    author = {Zimbra, David and Abbasi, Ahmed and Zeng, Daniel and Chen, Hsinchun},
    title = {The State-of-the-Art in Twitter Sentiment Analysis: A Review and Benchmark Evaluation},
    year = {2018},
    issue_date = {June 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {9},
    number = {2},
    issn = {2158-656X},
    url = {https://doi.org/10.1145/3185045},
    doi = {10.1145/3185045},
    abstract = {Twitter has emerged as a major social media platform and generated great interest from sentiment analysis researchers. Despite this attention, state-of-the-art Twitter sentiment analysis approaches perform relatively poorly with reported classification accuracies often below 70%, adversely impacting applications of the derived sentiment information. In this research, we investigate the unique challenges presented by Twitter sentiment analysis and review the literature to determine how the devised approaches have addressed these challenges. To assess the state-of-the-art in Twitter sentiment analysis, we conduct a benchmark evaluation of 28 top academic and commercial systems in tweet sentiment classification across five distinctive data sets. We perform an error analysis to uncover the causes of commonly occurring classification errors. To further the evaluation, we apply select systems in an event detection case study. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of the next generation of approaches.},
    journal = {ACM Trans. Manage. Inf. Syst.},
    month = {aug},
    articleno = {5},
    numpages = {29},
    keywords = {Sentiment analysis, benchmark evaluation, text mining, twitter, social media, natural language processing, opinion mining}
}

@article{Go_Bhayani_Huang_2009,
    added-at = {2013-04-09T13:26:10.000+0200},
    author = {Go, Alec and Bhayani, Richa and Huang, Lei},
    biburl = {https://www.bibsonomy.org/bibtex/27b69035cfbcccab96a665f33678c49bd/bsc},
    interhash = {2dc1e21bf5b3ebf7259154d92ec31146},
    intrahash = {7b69035cfbcccab96a665f33678c49bd},
    journal = {Processing},
    keywords = {analysis ba2013 sentiment twitter},
    pages = {1--6},
    timestamp = {2013-04-09T13:26:11.000+0200},
    title = {Twitter Sentiment Classification using Distant Supervision},
    url = {http://www.stanford.edu/~alecmgo/papers/TwitterDistantSupervision09.pdf},
    year = 2009
}

@book{thomas2020natural,
    title={Natural Language Processing with Spark NLP: Learning to Understand Text at Scale},
    author={Thomas, A.},
    isbn={9781492047766},
    lccn={2021278035},
    url={https://books.google.cl/books?id=sJw6zQEACAAJ},
    year={2020},
    publisher={O'Reilly Media}
}

@book{torres2019deep,
    title={Deep learning: introducci{\'o}n pr{\'a}ctica con Keras : segunda parte},
    author={Torres, J.},
    isbn={9781687473998},
    series={Watch this space},
    url={https://books.google.cl/books?id=c152zgEACAAJ},
    year={2019},
    publisher={Kindle Direct Publishing}
}

@misc{https://doi.org/10.48550/arxiv.1706.03762,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Attention Is All You Need},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1810.04805,
    doi = {10.48550/ARXIV.1810.04805},
    url = {https://arxiv.org/abs/1810.04805},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
    author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
    title     = {Learning Word Vectors for Sentiment Analysis},
    booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
    month     = {June},
    year      = {2011},
    address   = {Portland, Oregon, USA},
    publisher = {Association for Computational Linguistics},
    pages     = {142--150},
    url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{Olga2015Imagenet,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = {{ImageNet Large Scale Visual Recognition Challenge}},
    Year = {2015},
    journal   = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume={115},
    number={3},
    pages={211-252}
}

@misc{Peters2018Elmo,
    doi = {10.48550/ARXIV.1802.05365},
    url = {https://arxiv.org/abs/1802.05365},
    author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Deep contextualized word representations},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{radford2018improving,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
    year = {2018},
    journal= {OpenAI}
}

@article{Dang_2020,
    doi = {10.3390/electronics9030483},
    url = {https://doi.org/10.3390%2Felectronics9030483},
    year = 2020,
    month = {mar},
    publisher = {{MDPI} {AG}},
    volume = {9},
    number = {3},
    pages = {483},
    author = {Nhan Cach Dang and Mar{\'{\i}}a N. Moreno-Garc{\'{\i}}a and Fernando De la Prieta},
    title = {Sentiment Analysis Based on Deep Learning: A Comparative Study},
    journal = {Electronics}
}

@misc{n_2019, 
    title={IMDB dataset of 50K movie reviews. Recuperado de https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews. Recuperado el 22 de abril de 2022}, 
    url={https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews}, 
    journal={Kaggle}, 
    author={Lakshmipathi, N}, 
    year={2019}, 
    month={Mar}
} 
 
@INPROCEEDINGS{Bhavitha_sa,  
    author={Bhavitha, B. K. and Rodrigues, Anisha P. and Chiplunkar, Niranjan N.},  
    booktitle={2017 International Conference on Inventive Communication and Computational Technologies (ICICCT)},   
    title={Comparative study of machine learning techniques in sentimental analysis},  
    year={2017},  
    volume={},  
    number={},  
    pages={216-221},  
    doi={10.1109/ICICCT.2017.7975191}
}
 
@misc{Ruder_NLP-progress_2022,
    author = {Ruder, Sebastian},
    doi = {10.5281/zenodo.1234},
    month = {2},
    title = {{NLP-progress}},
    url = {https://nlpprogress.com/},
    version = {1.0.0},
    year = {2022}
}

@misc{XLNET_https://doi.org/10.48550/arxiv.1906.08237,
    doi = {10.48550/ARXIV.1906.08237},
    url = {https://arxiv.org/abs/1906.08237},
    author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    publisher = {arXiv},
    year = {2019},
    copyright = {Creative Commons Attribution 4.0 International}
}

@misc{FTBERTCLAS_https://doi.org/10.48550/arxiv.1905.05583,
    doi = {10.48550/ARXIV.1905.05583},
    url = {https://arxiv.org/abs/1905.05583},
    author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {How to Fine-Tune BERT for Text Classification?},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@software{Luis_Arturo_09MIAR-TFM_2022,
    author = {Luis Arturo, Izaguirre Viera},
    month = {4},
    title = {{09MIAR-TFM}},
    version = {0.0.1},
    year = {2022},
    url = {https://github.com/lsizaguirre/09MIAR-TFM}
}

@article{Fiok_2021_TextGuideLong,
    doi = {10.1109/access.2021.3099758},
    url = {https://doi.org/10.1109%2Faccess.2021.3099758},
    year = 2021,
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume = {9},
    pages = {105439--105450},
    author = {Krzysztof Fiok and Waldemar Karwowski and Edgar Gutierrez-Franco and Mohammad Reza Davahli and Maciej Wilamowski and Tareq Ahram and Awad Al-Juaid and Jozef Zurada},
    title = {Text Guide: Improving the Quality of Long Text Classification by a Text Selection Method Based on Feature Importance},
    journal = {{IEEE} Access}
}

@article{articleFang2020LSTM,
    author = {Fang Hao and Jiang Man and Lu Jingyan and Qin Gong and Tao and Tang Peifu and Yunjun Yan and Yun, Luo and Liu},
    year = {2020},
    month = {01},
    pages = {256},
    title = {A LSTM Algorithm Estimating Pseudo Measurements for Aiding INS during GNSS Signal Outages},
    volume = {12},
    journal = {Remote Sensing},
    doi = {10.3390/rs12020256}
}

@article{articleSong2020LSTM,
    author = {Song, Rui and Ding, James and Wu, and Liu, and Chu,},
    year = {2019},
    month = {12},
    pages = {109},
    title = {Flash Flood Forecasting Based on Long Short-Term Memory Networks},
    volume = {12},
    journal = {Water},
    doi = {10.3390/w12010109}
}

@misc{articleBrownGPT3,
    doi = {10.48550/ARXIV.2005.14165},
    url = {https://arxiv.org/abs/2005.14165},
    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Language Models are Few-Shot Learners},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ALBERT_https://doi.org/10.48550/arxiv.1909.11942,
    doi = {10.48550/ARXIV.1909.11942},
    url = {https://arxiv.org/abs/1909.11942},
    author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ROBERTA_https://doi.org/10.48550/arxiv.1907.11692,
    doi = {10.48550/ARXIV.1907.11692},
    url = {https://arxiv.org/abs/1907.11692},
    author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ERNIE_https://doi.org/10.48550/arxiv.1904.09223,
    doi = {10.48550/ARXIV.1904.09223},
    url = {https://arxiv.org/abs/1904.09223},
    author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {ERNIE: Enhanced Representation through Knowledge Integration},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DISTILBERT_https://doi.org/10.48550/arxiv.1910.01108,
    doi = {10.48550/ARXIV.1910.01108},
    url = {https://arxiv.org/abs/1910.01108},
    author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{chris_mccormick_and_nick_ryan_bert_2019,
    title = {{BERT} {Fine}-{Tuning} {Tutorial} with {PyTorch}. {Chris} {McCormick} . Retrieved from http://www.mccormickml.com},
    url = {https://mccormickml.com/2019/07/22/BERT-fine-tuning/},
    urldate = {2022-04-30},
    author = {Chris McCormick {and} Nick Ryan},
    year = {2019},
}

@conference{csedu20,
    author={Hadi Ghavidel. and Amal Zouaq. and Michel Desmarais.},
    title={Using BERT and XLNET for the Automatic Short Answer Grading Task},
    booktitle={Proceedings of the 12th International Conference on Computer Supported Education - Volume 1: CSEDU,},
    year={2020},
    pages={58-67},
    publisher={SciTePress},
    organization={INSTICC},
    doi={10.5220/0009422400580067},
    isbn={978-989-758-417-6},
    issn={2184-5026},
}

@misc{Topal_2021_exploring_https://doi.org/10.48550/arxiv.2102.08036,
    doi = {10.48550/ARXIV.2102.08036},
    url = {https://arxiv.org/abs/2102.08036},
    author = {Topal, M. Onat and Bas, Anil and van Heerden, Imke},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet},
    publisher = {arXiv},
    year = {2021},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ULMFit_https://doi.org/10.48550/arxiv.1801.06146,
    doi = {10.48550/ARXIV.1801.06146},
    url = {https://arxiv.org/abs/1801.06146},
    author = {Howard, Jeremy and Ruder, Sebastian},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Universal Language Model Fine-tuning for Text Classification},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SQUADv1_https://doi.org/10.48550/arxiv.1606.05250,
    doi = {10.48550/ARXIV.1606.05250},
    url = {https://arxiv.org/abs/1606.05250},
    author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SQUADv2_https://doi.org/10.48550/arxiv.1806.03822,
    doi = {10.48550/ARXIV.1806.03822},
    url = {https://arxiv.org/abs/1806.03822},
    author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Know What You Don't Know: Unanswerable Questions for SQuAD},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{CoQA_https://doi.org/10.48550/arxiv.1808.07042,
    doi = {10.48550/ARXIV.1808.07042},
    url = {https://arxiv.org/abs/1808.07042},
    author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {CoQA: A Conversational Question Answering Challenge},
    publisher = {arXiv},
    year = {2018},
    copyright = {Creative Commons Attribution 4.0 International}
}

@misc{BERT_baseline_for_NQ_https://doi.org/10.48550/arxiv.1901.08634,
    doi = {10.48550/ARXIV.1901.08634},
    url = {https://arxiv.org/abs/1901.08634},
    author = {Alberti, Chris and Lee, Kenton and Collins, Michael},
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {A BERT Baseline for the Natural Questions},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SQuAD_Comparison_https://doi.org/10.48550/arxiv.2005.11313,
    doi = {10.48550/ARXIV.2005.11313},
    url = {https://arxiv.org/abs/2005.11313},
    author = {Patel, Devshree and Raval, Param and Parikh, Ratnam and Shastri, Yesha},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Comparative Study of Machine Learning Models and BERT on SQuAD},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{LSTM_Olah_2015,
    title	= {Understanding LSTM Networks},
    author	= {Christopher Olah},
    year	= {2015},
    URL	= {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}

@misc{PANG_LEE_SST_https://doi.org/10.48550/arxiv.cs/0506075,
    doi = {10.48550/ARXIV.CS/0506075},
    url = {https://arxiv.org/abs/cs/0506075},
    author = {Pang, Bo and Lee, Lillian},
    keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7; I.2.6},
    title = {Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
    publisher = {arXiv},
    year = {2005},
    copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}
